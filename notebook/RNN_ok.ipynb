{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4PkWsriNjr9",
        "outputId": "405b2e4a-07a9-48ca-fe60-83d9d249875a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITZPGuYejS0_",
        "outputId": "869a82d6-f298-4e22-fb64-f26e6bc7023c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR2Fu5sGjWq3",
        "outputId": "9861f56f-91f1-4ee1-d8bf-25e843e1dead"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M8vChdjNTXh",
        "outputId": "25f7e1a0-3efa-49ac-8109-c5d5f1d36ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando preprocesamiento de texto...\n",
            "Tokenizando textos...\n",
            "Cargando embeddings de GloVe...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-13 20:55:19,321] A new study created in memory with name: no-name-b84abcef-f8c0-494e-a5ff-d1ade0d5aee3\n",
            "[I 2024-11-13 20:55:55,795] Trial 0 finished with value: 0.6759993433952332 and parameters: {'conv_filters': 64, 'lstm_units_1': 128, 'lstm_units_2': 51, 'dense_units_1': 163, 'dense_units_2': 68, 'dropout_1': 0.3734897454159435, 'dropout_2': 0.15378940697279803, 'learning_rate': 0.0047729130195821425, 'batch_size': 64}. Best is trial 0 with value: 0.6759993433952332.\n",
            "[I 2024-11-13 20:56:39,157] Trial 1 finished with value: 0.6920093695322672 and parameters: {'conv_filters': 102, 'lstm_units_1': 78, 'lstm_units_2': 53, 'dense_units_1': 137, 'dense_units_2': 48, 'dropout_1': 0.5364619958829588, 'dropout_2': 0.13608967132728858, 'learning_rate': 0.00037042433861917003, 'batch_size': 16}. Best is trial 1 with value: 0.6920093695322672.\n",
            "[I 2024-11-13 20:57:18,583] Trial 2 finished with value: 0.7009734312693278 and parameters: {'conv_filters': 165, 'lstm_units_1': 53, 'lstm_units_2': 26, 'dense_units_1': 123, 'dense_units_2': 59, 'dropout_1': 0.520286087510389, 'dropout_2': 0.11446850543345169, 'learning_rate': 0.0013294828631920292, 'batch_size': 16}. Best is trial 2 with value: 0.7009734312693278.\n",
            "[I 2024-11-13 20:57:55,485] Trial 3 finished with value: 0.6800033648808798 and parameters: {'conv_filters': 211, 'lstm_units_1': 127, 'lstm_units_2': 53, 'dense_units_1': 158, 'dense_units_2': 107, 'dropout_1': 0.436031133551151, 'dropout_2': 0.11846763838290396, 'learning_rate': 0.0027395029569877417, 'batch_size': 32}. Best is trial 2 with value: 0.7009734312693278.\n",
            "[I 2024-11-13 20:58:35,865] Trial 4 finished with value: 0.6069932182629904 and parameters: {'conv_filters': 201, 'lstm_units_1': 106, 'lstm_units_2': 29, 'dense_units_1': 99, 'dense_units_2': 82, 'dropout_1': 0.32554542321578517, 'dropout_2': 0.23557178588197186, 'learning_rate': 0.007491073752903163, 'batch_size': 32}. Best is trial 2 with value: 0.7009734312693278.\n",
            "[I 2024-11-13 20:59:20,446] Trial 5 finished with value: 0.6949644088745117 and parameters: {'conv_filters': 77, 'lstm_units_1': 116, 'lstm_units_2': 36, 'dense_units_1': 182, 'dense_units_2': 123, 'dropout_1': 0.22372096383755724, 'dropout_2': 0.25139680141049126, 'learning_rate': 0.00011610891678969611, 'batch_size': 32}. Best is trial 2 with value: 0.7009734312693278.\n",
            "[I 2024-11-13 20:59:59,084] Trial 6 finished with value: 0.6899624268213908 and parameters: {'conv_filters': 91, 'lstm_units_1': 120, 'lstm_units_2': 29, 'dense_units_1': 208, 'dense_units_2': 87, 'dropout_1': 0.5322387503458372, 'dropout_2': 0.2182866125641114, 'learning_rate': 0.0002087239732693417, 'batch_size': 32}. Best is trial 2 with value: 0.7009734312693278.\n",
            "[I 2024-11-13 21:00:27,587] Trial 7 finished with value: 0.720013419787089 and parameters: {'conv_filters': 115, 'lstm_units_1': 62, 'lstm_units_2': 54, 'dense_units_1': 114, 'dense_units_2': 113, 'dropout_1': 0.2932190281401975, 'dropout_2': 0.1916148429066561, 'learning_rate': 0.00544516354912993, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:00:57,698] Trial 8 finished with value: 0.6939993898073832 and parameters: {'conv_filters': 209, 'lstm_units_1': 57, 'lstm_units_2': 39, 'dense_units_1': 99, 'dense_units_2': 112, 'dropout_1': 0.3557461136438488, 'dropout_2': 0.38167516220310804, 'learning_rate': 0.00038947409602144704, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:01:37,542] Trial 9 finished with value: 0.6860063672065735 and parameters: {'conv_filters': 69, 'lstm_units_1': 75, 'lstm_units_2': 62, 'dense_units_1': 103, 'dense_units_2': 118, 'dropout_1': 0.257770188020772, 'dropout_2': 0.269859541798277, 'learning_rate': 0.000543416630810293, 'batch_size': 16}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:02:04,716] Trial 10 finished with value: 0.6990103920300802 and parameters: {'conv_filters': 132, 'lstm_units_1': 32, 'lstm_units_2': 63, 'dense_units_1': 233, 'dense_units_2': 96, 'dropout_1': 0.2868060023480495, 'dropout_2': 0.3240763732820364, 'learning_rate': 0.00192917738761073, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:02:47,756] Trial 11 finished with value: 0.6830033659934998 and parameters: {'conv_filters': 164, 'lstm_units_1': 52, 'lstm_units_2': 18, 'dense_units_1': 68, 'dense_units_2': 58, 'dropout_1': 0.45962307045549416, 'dropout_2': 0.17799611855694675, 'learning_rate': 0.0015606323829252817, 'batch_size': 16}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:03:20,454] Trial 12 finished with value: 0.706032375494639 and parameters: {'conv_filters': 157, 'lstm_units_1': 58, 'lstm_units_2': 17, 'dense_units_1': 133, 'dense_units_2': 39, 'dropout_1': 0.5876885532814969, 'dropout_2': 0.1071992849584851, 'learning_rate': 0.0009869112251642065, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:03:50,618] Trial 13 finished with value: 0.6820353269577026 and parameters: {'conv_filters': 127, 'lstm_units_1': 93, 'lstm_units_2': 45, 'dense_units_1': 68, 'dense_units_2': 40, 'dropout_1': 0.5744837907375817, 'dropout_2': 0.18964013945074795, 'learning_rate': 0.0007844328813192885, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:04:17,645] Trial 14 finished with value: 0.6959983905156454 and parameters: {'conv_filters': 253, 'lstm_units_1': 67, 'lstm_units_2': 18, 'dense_units_1': 133, 'dense_units_2': 36, 'dropout_1': 0.45174363726088196, 'dropout_2': 0.17673248400429453, 'learning_rate': 0.004619144163520987, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:04:48,896] Trial 15 finished with value: 0.6950063904126486 and parameters: {'conv_filters': 132, 'lstm_units_1': 36, 'lstm_units_2': 47, 'dense_units_1': 179, 'dense_units_2': 98, 'dropout_1': 0.5934614984156066, 'dropout_2': 0.28457346931056254, 'learning_rate': 0.00864373267109398, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:05:13,938] Trial 16 finished with value: 0.6650423010190328 and parameters: {'conv_filters': 184, 'lstm_units_1': 89, 'lstm_units_2': 59, 'dense_units_1': 150, 'dense_units_2': 70, 'dropout_1': 0.20170509133489717, 'dropout_2': 0.10247896175571927, 'learning_rate': 0.0030438282916457108, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:05:41,860] Trial 17 finished with value: 0.7180323998133341 and parameters: {'conv_filters': 109, 'lstm_units_1': 64, 'lstm_units_2': 36, 'dense_units_1': 120, 'dense_units_2': 128, 'dropout_1': 0.3131896203586761, 'dropout_2': 0.20940667921168316, 'learning_rate': 0.0008117186361944908, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:06:08,705] Trial 18 finished with value: 0.7069944143295288 and parameters: {'conv_filters': 109, 'lstm_units_1': 43, 'lstm_units_2': 35, 'dense_units_1': 114, 'dense_units_2': 127, 'dropout_1': 0.30550535316775174, 'dropout_2': 0.21027814075174486, 'learning_rate': 0.0006763312865833334, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n",
            "[I 2024-11-13 21:06:45,468] Trial 19 finished with value: 0.6990343729654948 and parameters: {'conv_filters': 116, 'lstm_units_1': 68, 'lstm_units_2': 44, 'dense_units_1': 89, 'dense_units_2': 103, 'dropout_1': 0.2595080761829555, 'dropout_2': 0.3320204937828155, 'learning_rate': 0.00021305441162391929, 'batch_size': 64}. Best is trial 7 with value: 0.720013419787089.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 129ms/step - accuracy: 0.5048 - auc: 0.5227 - loss: 0.6919 - precision: 0.3978 - recall: 0.2383 - val_accuracy: 0.7100 - val_auc: 0.7786 - val_loss: 0.5773 - val_precision: 0.6735 - val_recall: 0.7174\n",
            "Epoch 2/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.7195 - auc: 0.7438 - loss: 0.5866 - precision: 0.7523 - recall: 0.5656 - val_accuracy: 0.6900 - val_auc: 0.7566 - val_loss: 0.6071 - val_precision: 0.6471 - val_recall: 0.7174\n",
            "Epoch 3/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7730 - auc: 0.8546 - loss: 0.4812 - precision: 0.8191 - recall: 0.6586 - val_accuracy: 0.7400 - val_auc: 0.7955 - val_loss: 0.6068 - val_precision: 0.8125 - val_recall: 0.5652\n",
            "Epoch 4/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8254 - auc: 0.9210 - loss: 0.3818 - precision: 0.8992 - recall: 0.7225 - val_accuracy: 0.6800 - val_auc: 0.7637 - val_loss: 0.6121 - val_precision: 0.6458 - val_recall: 0.6739\n",
            "Epoch 5/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8686 - auc: 0.9363 - loss: 0.3291 - precision: 0.8854 - recall: 0.8137 - val_accuracy: 0.6900 - val_auc: 0.7482 - val_loss: 0.6553 - val_precision: 0.6667 - val_recall: 0.6522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D, Conv1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import optuna\n",
        "import logging\n",
        "import spacy\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Descargar recursos NLTK\n",
        "nltk.download('punkt')  # Cambiado de punkt_tab a punkt\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Configuración de parámetros base\n",
        "MAX_WORDS = 15000\n",
        "MAX_LEN = 128\n",
        "EMBEDDING_DIM = 300  # Cambiado a 300 para coincidir con GloVe\n",
        "\n",
        "def preprocess_text(text_series):\n",
        "    \"\"\"\n",
        "    Preprocesamiento mejorado con manejo especial de palabras ofensivas.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english')) - {'no', 'not', 'hate', 'against', 'racist', 'abuse', 'toxic'}\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        # Convertir a minúsculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # Preservar ciertas palabras compuestas ofensivas\n",
        "        text = text.replace('son of a bitch', 'sonofabitch')\n",
        "        text = text.replace('f u c k', 'fuck')\n",
        "        text = text.replace('b i t c h', 'bitch')\n",
        "\n",
        "        # Eliminar URLs\n",
        "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "        # Preservar algunos caracteres especiales que pueden indicar toxicidad\n",
        "        text = re.sub(r'[^a-zA-Z\\s!?*#@$]', '', text)\n",
        "\n",
        "        # Eliminar espacios extras\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Lematización y eliminación de stopwords\n",
        "        words = text.split()\n",
        "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    return text_series.apply(clean_text)\n",
        "\n",
        "def prepare_data(df):\n",
        "    \"\"\"\n",
        "    Prepara los datos para el entrenamiento.\n",
        "    \"\"\"\n",
        "    # Preprocesar texto\n",
        "    print(\"Iniciando preprocesamiento de texto...\")\n",
        "    processed_texts = preprocess_text(df['Text'])\n",
        "\n",
        "    # Tokenización y creación de secuencias\n",
        "    print(\"Tokenizando textos...\")\n",
        "    tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(processed_texts)\n",
        "    sequences = tokenizer.texts_to_sequences(processed_texts)\n",
        "    X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "    # Preparar etiquetas\n",
        "    target_columns = ['IsToxic', 'IsAbusive', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist']\n",
        "    df['IsOffensive'] = df[target_columns].any(axis=1)\n",
        "    y = df['IsOffensive'].astype(int)\n",
        "\n",
        "    # Cargar los embeddings de GloVe\n",
        "    print(\"Cargando embeddings de GloVe...\")\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
        "\n",
        "    # Crear matriz de embeddings\n",
        "    embedding_matrix = np.zeros((MAX_WORDS + 1, EMBEDDING_DIM))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i < MAX_WORDS:  # Solo procesamos hasta MAX_WORDS\n",
        "            try:\n",
        "                embedding_matrix[i] = glove_model[word]\n",
        "            except KeyError:\n",
        "                continue  # Si la palabra no está en GloVe, dejamos el vector en ceros\n",
        "\n",
        "    return X, y, tokenizer, embedding_matrix\n",
        "\n",
        "def create_model_tuned(vocab_size, num_labels, params, embedding_matrix):\n",
        "    \"\"\"\n",
        "    Versión mejorada del modelo con parámetros optimizados y embeddings\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False),\n",
        "        Conv1D(params.get('conv_filters', 128), 5, activation='relu'),\n",
        "        Bidirectional(LSTM(params.get('lstm_units_1', 64), return_sequences=True)),\n",
        "        Bidirectional(LSTM(params.get('lstm_units_2', 32), return_sequences=True)),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(params.get('dense_units_1', 128), activation='relu'),\n",
        "        Dropout(params.get('dropout_1', 0.5)),\n",
        "        Dense(params.get('dense_units_2', 64), activation='relu'),\n",
        "        Dropout(params.get('dropout_2', 0.3)),\n",
        "        Dense(num_labels, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=params.get('learning_rate', 0.001))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def objective(trial, X, y, tokenizer, embedding_matrix):\n",
        "    \"\"\"\n",
        "    Función objetivo para Optuna\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        'conv_filters': trial.suggest_int('conv_filters', 64, 256),\n",
        "        'lstm_units_1': trial.suggest_int('lstm_units_1', 32, 128),\n",
        "        'lstm_units_2': trial.suggest_int('lstm_units_2', 16, 64),\n",
        "        'dense_units_1': trial.suggest_int('dense_units_1', 64, 256),\n",
        "        'dense_units_2': trial.suggest_int('dense_units_2', 32, 128),\n",
        "        'dropout_1': trial.suggest_float('dropout_1', 0.2, 0.6),\n",
        "        'dropout_2': trial.suggest_float('dropout_2', 0.1, 0.4),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
        "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "    }\n",
        "\n",
        "    # Validación cruzada para evaluación más robusta\n",
        "    kf = KFold(n_splits=3, shuffle=True)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        model = create_model_tuned(MAX_WORDS + 1, 1, params, embedding_matrix)\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=10,\n",
        "            batch_size=params['batch_size'],\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        scores.append(history.history['val_accuracy'][-1])\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "def train_with_optimization(df, n_trials=20):\n",
        "    \"\"\"\n",
        "    Pipeline de entrenamiento con optimización de hiperparámetros\n",
        "    \"\"\"\n",
        "    logging.info(\"Preparando datos...\")\n",
        "    X, y, tokenizer, embedding_matrix = prepare_data(df)\n",
        "\n",
        "    logging.info(\"Iniciando optimización de hiperparámetros...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "\n",
        "    try:\n",
        "        study.optimize(lambda trial: objective(trial, X, y, tokenizer, embedding_matrix),\n",
        "                      n_trials=n_trials)\n",
        "\n",
        "        best_params = study.best_params\n",
        "        logging.info(\"Mejores hiperparámetros encontrados:\")\n",
        "        for param, value in best_params.items():\n",
        "            logging.info(f\"{param}: {value}\")\n",
        "\n",
        "        logging.info(\"Entrenando modelo final...\")\n",
        "        final_model = create_model_tuned(MAX_WORDS + 1, 1, best_params, embedding_matrix)\n",
        "        history = final_model.fit(\n",
        "            X, y,\n",
        "            epochs=15,\n",
        "            batch_size=best_params.get('batch_size', 32),\n",
        "            validation_split=0.1,\n",
        "            callbacks=[EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)]\n",
        "        )\n",
        "\n",
        "        return final_model, history\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error durante la optimización: {str(e)}\")\n",
        "        default_params = {\n",
        "            'conv_filters': 128,\n",
        "            'lstm_units_1': 64,\n",
        "            'lstm_units_2': 32,\n",
        "            'dense_units_1': 128,\n",
        "            'dense_units_2': 64,\n",
        "            'dropout_1': 0.5,\n",
        "            'dropout_2': 0.3,\n",
        "            'learning_rate': 0.001,\n",
        "            'batch_size': 32\n",
        "        }\n",
        "        logging.info(\"Usando parámetros por defecto debido al error...\")\n",
        "        final_model = create_model_tuned(MAX_WORDS + 1, 1, default_params, embedding_matrix)\n",
        "        history = final_model.fit(\n",
        "            X, y,\n",
        "            epochs=15,\n",
        "            batch_size=default_params['batch_size'],\n",
        "            validation_split=0.1,\n",
        "            callbacks=[EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)]\n",
        "        )\n",
        "\n",
        "        return final_model, history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Asegúrate de que el archivo CSV existe y tiene las columnas correctas\n",
        "    df = pd.read_csv('youtoxic_english_1000.csv')\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Iniciando entrenamiento con optimización...\")\n",
        "        final_model, history = train_with_optimization(df, n_trials=20)\n",
        "\n",
        "        # Guardar el modelo\n",
        "        final_model.save('final_model.h5')\n",
        "        logging.info(\"Entrenamiento completado y modelo guardado.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error durante el entrenamiento: {str(e)}\")"
      ]
    }
  ]
}